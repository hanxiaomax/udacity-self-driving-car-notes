(window.webpackJsonp=window.webpackJsonp||[]).push([[9],{190:function(t,a,s){t.exports=s.p+"assets/img/15383615644699.00b6b893.jpg"},191:function(t,a,s){t.exports=s.p+"assets/img/15383614215865.58bb314d.jpg"},192:function(t,a,s){t.exports=s.p+"assets/img/15383604396008.3cdd8d81.jpg"},193:function(t,a,s){t.exports=s.p+"assets/img/15383601524457.e49095d2.jpg"},194:function(t,a,s){t.exports=s.p+"assets/img/15383596251413.ad24e96e.jpg"},195:function(t,a,s){t.exports=s.p+"assets/img/15383589937717.250065dc.jpg"},196:function(t,a,s){t.exports=s.p+"assets/img/15383590668383.9bdf08ad.jpg"},197:function(t,a,s){t.exports=s.p+"assets/img/15381474001238.b8a7f9a6.jpg"},198:function(t,a,s){t.exports=s.p+"assets/img/15381465295268.77134430.jpg"},199:function(t,a,s){t.exports=s.p+"assets/img/15381463702675.ec33f8a9.jpg"},200:function(t,a,s){t.exports=s.p+"assets/img/15381451572296.66e98552.jpg"},201:function(t,a,s){t.exports=s.p+"assets/img/15381430330924.955207e0.jpg"},202:function(t,a,s){t.exports=s.p+"assets/img/15381427269230.76c33031.jpg"},203:function(t,a,s){t.exports=s.p+"assets/img/15381422748914.83da37f6.jpg"},204:function(t,a,s){t.exports=s.p+"assets/img/15381422049236.12e7ad51.jpg"},205:function(t,a,s){t.exports=s.p+"assets/img/15381402524801.bb2ad1c1.jpg"},206:function(t,a,s){t.exports=s.p+"assets/img/15380921039335.5e0e19cc.jpg"},290:function(t,a,s){"use strict";s.r(a);var n=[function(){var t=this,a=t.$createElement,n=t._self._c||a;return n("div",{staticClass:"content"},[n("h1",{attrs:{id:"卷积神经网络"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#卷积神经网络","aria-hidden":"true"}},[t._v("#")]),t._v(" 卷积神经网络")]),n("ul",[n("li",[t._v("如果我们已经知晓数据集的某些特征，学习可以变得更容易，例如，如果我们知道，颜色不是决定字母的因素，我们则可以采取灰度图像而非RGB图像")]),n("li",[t._v("如果我们想判断图片中是否有猫，不指定猫的位置会让学习更简单")]),n("li",[t._v("如果我们的神经网络试图理解一个句子，句子中反复出现的单词kitten，基本上和它出现的位置无关")])]),n("h2",{attrs:{id:"_1-统计不变量与贡献权重"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_1-统计不变量与贡献权重","aria-hidden":"true"}},[t._v("#")]),t._v(" 1. 统计不变量与贡献权重")]),n("p",[t._v("在上面两个例子中，目标中有部分信息可以反复利用，而不需要重新学习。我们可以通过"),n("strong",[t._v("共享权重")]),t._v("来利用这一点。\n"),n("img",{attrs:{src:s(206),alt:""}})]),n("p",[t._v("当我们知道，两个输入中可能包含相同的信息时，我们可以共享它们的权重，并且利用这些输入，共同训练权重。\n"),n("strong",[t._v("统计不变量")]),t._v("指的是基本上不会随时空变换的统计量。")]),n("h2",{attrs:{id:"_2-卷积神经网络"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_2-卷积神经网络","aria-hidden":"true"}},[t._v("#")]),t._v(" 2. 卷积神经网络")]),n("h3",{attrs:{id:"_1-术语"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_1-术语","aria-hidden":"true"}},[t._v("#")]),t._v(" 1. 术语")]),n("ul",[n("li",[t._v("过滤器")]),n("li",[t._v("步长")]),n("li",[t._v("patch")]),n("li",[t._v("Padding")])]),n("h3",{attrs:{id:"_2-过滤器（kernel）"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_2-过滤器（kernel）","aria-hidden":"true"}},[t._v("#")]),t._v(" 2. 过滤器（kernel）")]),n("p",[n("img",{attrs:{src:s(205),alt:""}})]),n("p",[t._v("过滤器的深度是指过滤器的个数，每个过滤器提取特定的特征，那么特征的数量就是深度，假设深度为k，则该patch在下一层连接k个感知元。")]),n("p",[t._v("为什么要连接多个感知元呢？因为一个patch可能有多个我们感兴趣的特征。")]),n("h3",{attrs:{id:"_3-feature-map-sizes"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_3-feature-map-sizes","aria-hidden":"true"}},[t._v("#")]),t._v(" 3. Feature Map Sizes")]),n("p",[n("img",{attrs:{src:s(204),alt:""}}),n("img",{attrs:{src:s(203),alt:""}})]),n("h3",{attrs:{id:"_4-padding"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_4-padding","aria-hidden":"true"}},[t._v("#")]),t._v(" 4. Padding")]),n("p",[n("img",{attrs:{src:s(202),alt:""}})]),n("p",[n("code",[t._v("5*5")]),t._v("的grid中，创建一个"),n("code",[t._v("3*3")]),t._v("的kernel，步长为1 的情况下，则下一层的维度是"),n("code",[t._v("3*3")]),t._v("（在任意方向只能通过移动，创建3个patch）")]),n("p",[t._v("为了不减少维度，我们可以对grid进行补0操作，使的我们能够创建足够多的patch来保持维度不变\n"),n("img",{attrs:{src:s(201),alt:""}})]),n("h3",{attrs:{id:"_5-卷积层尺寸"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_5-卷积层尺寸","aria-hidden":"true"}},[t._v("#")]),t._v(" 5. 卷积层尺寸")]),n("p",[t._v("我们必须能够知道，每层网络的大小，以便知晓模型的规模，在尺寸和性能上做出取舍")]),n("ul",[n("li",[t._v("输入层 width 为 W ， height 为 H")]),n("li",[t._v("卷积层的过滤器大小为 F")]),n("li",[t._v("步长为 S")]),n("li",[t._v("padding 为 P")]),n("li",[t._v("深度，即过滤器的个数为 K")])]),n("p",[t._v("则，下一层的尺寸为：")]),n("ul",[n("li",[n("code",[t._v("W_out =[ (W−F+2P)/S] + 1")])]),n("li",[n("code",[t._v("H_out = [(H-F+2P)/S] + 1")])]),n("li",[n("code",[t._v("D_out = K")])]),n("li",[t._v("下一层的尺寸为 "),n("code",[t._v("W_out * H_out * D_out")])])]),n("h4",{attrs:{id:"练习"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#练习","aria-hidden":"true"}},[t._v("#")]),t._v(" 练习")]),n("p",[t._v("Setup\nH = height, W = width, D = depth")]),n("ul",[n("li",[t._v("We have an input of shape 32x32x3 (HxWxD)")]),n("li",[t._v("20 filters of shape 8x8x3 (HxWxD)")]),n("li",[t._v("A stride of 2 for both the height and width (S)")]),n("li",[t._v("With padding of size 1 (P)")])]),n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("D_out = K = 20\nW_out =[ (32−8+2)/2] + 1 = 14\nH_out =[ (32−8+2)/2] + 1 = 14\n")])]),n("p",[t._v("但是下面一段代码，conv的输出维度是"),n("code",[t._v("[1, 16, 16, 20]")]),t._v("，而不是"),n("code",[t._v("[1, 14, 14, 20]")]),t._v("，这和tf添加padding的机制有关，可以看"),n("a",{attrs:{href:"https://www.tensorflow.org/api_guides/python/nn#Convolution",target:"_blank",rel:"noopener noreferrer"}},[t._v("这里")])]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{attrs:{class:"token builtin"}},[t._v("input")]),t._v(" "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("placeholder"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("float32"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token boolean"}},[t._v("None")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("32")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("32")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("3")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfilter_weights "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("truncated_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token number"}},[t._v("8")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("8")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("3")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("20")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{attrs:{class:"token comment"}},[t._v("# (height, width, input_depth, output_depth)")]),t._v("\nfilter_bias "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token number"}},[t._v("20")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nstrides "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{attrs:{class:"token comment"}},[t._v("# (batch, height, width, depth)")]),t._v("\npadding "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token string"}},[t._v("'SAME'")]),t._v("\nconv "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv2d"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token builtin"}},[t._v("input")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" filter_weights"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" strides"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" padding"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{attrs:{class:"token operator"}},[t._v("+")]),t._v(" filter_bias\n")])]),n("p",[t._v("简单来讲：\n"),n("strong",[t._v("SAME")]),t._v(" Padding:\nout_height = ceil(float(in_height) / float(strides[1]))\nout_width = ceil(float(in_width) / float(strides[2]))\n"),n("strong",[t._v("VALID")]),t._v(" Padding\nout_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\nout_width = ceil(float(in_width - filter_width + 1) / float(strides[2]))")]),n("h4",{attrs:{id:"参数个数计算"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#参数个数计算","aria-hidden":"true"}},[t._v("#")]),t._v(" 参数个数计算")]),n("p",[t._v("按照上述的输入输出结果（Output Layer 14x14x20 (HxWxD)），计算参数的个数：")]),n("ul",[n("li",[t._v("如果不共享参数，则每一层参数个数为"),n("code",[t._v("(8 * 8 * 3 + 1) * (14 * 14 * 20) = 756560")]),t._v("（+1是bias）")]),n("li",[t._v("如果是共享参数的CNN，则参数个数为"),n("code",[t._v("(8 * 8 * 3 + 1) * 20 = 3840 + 20 = 3860")])])]),n("p",[t._v("我们可以看出，共享参数则不需要在输出层的每个W*H上再去学习，只需要乘上深度即可。")]),n("h3",{attrs:{id:"_6-卷积神经网络的可视化"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_6-卷积神经网络的可视化","aria-hidden":"true"}},[t._v("#")]),t._v(" 6. 卷积神经网络的可视化")]),n("div",{staticClass:"tip custom-block"},[n("p",{staticClass:"custom-block-title"},[t._v("文献")]),n("p",[n("a",{attrs:{href:"http://www.matthewzeiler.com/wp-content/uploads/2017/07/eccv2014.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("LNCS 8689 - Visualizing and Understanding Convolutional Networks")])])]),n("h4",{attrs:{id:"第一层"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#第一层","aria-hidden":"true"}},[t._v("#")]),t._v(" 第一层")]),n("p",[n("img",{attrs:{src:s(200),alt:""}})]),n("p",[t._v("So, the first layer of our CNN clearly picks out very simple shapes and patterns like lines and blobs.")]),n("h4",{attrs:{id:"第二层"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#第二层","aria-hidden":"true"}},[t._v("#")]),t._v(" 第二层")]),n("p",[n("img",{attrs:{src:s(199),alt:""}})]),n("p",[t._v("As you see in the image above, the second layer of the CNN recognizes circles (second row, second column), stripes (first row, second column), and rectangles (bottom right).")]),n("h4",{attrs:{id:"第三层"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#第三层","aria-hidden":"true"}},[t._v("#")]),t._v(" 第三层")]),n("p",[n("img",{attrs:{src:s(198),alt:""}})]),n("p",[t._v("The third layer picks out complex combinations of features from the second layer. These include things like grids, and honeycombs (top left), wheels (second row, second column), and even faces (third row, third column).")]),n("h4",{attrs:{id:"第五层"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#第五层","aria-hidden":"true"}},[t._v("#")]),t._v(" 第五层")]),n("p",[n("img",{attrs:{src:s(197),alt:""}})]),n("h3",{attrs:{id:"_4-池化"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_4-池化","aria-hidden":"true"}},[t._v("#")]),t._v(" 4. 池化")]),n("p",[t._v("通过调整步长，我们可以降低图像的采样率，即移除一些图像信息，此外我们还可以将相邻的卷积进行聚合，得到一个卷积值来降低采样率，这个过程叫做"),n("strong",[t._v("池化")]),t._v("。池化用来减少输出尺寸，避免过拟合。")]),n("ul",[n("li",[t._v("最大池化")]),n("li",[t._v("平均池化")])]),n("h4",{attrs:{id:"最大池化的特点："}},[n("a",{staticClass:"header-anchor",attrs:{href:"#最大池化的特点：","aria-hidden":"true"}},[t._v("#")]),t._v(" 最大池化的特点：")]),n("ul",[n("li",[t._v("不会增加参数，不必担心过拟合")]),n("li",[t._v("通常会提高模型准确率")]),n("li",[t._v("计算量更大，因为步长小了")]),n("li",[t._v("超参数增加：池区大小、池化步幅")])]),n("p",[t._v("将池区中的卷积去最大值保留\n"),n("img",{attrs:{src:s(196),alt:""}})]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("conv_layer "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv2d"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token builtin"}},[t._v("input")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weight"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" strides"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" padding"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token string"}},[t._v("'SAME'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nconv_layer "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bias_add"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("conv_layer"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" bias"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nconv_layer "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("relu"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("conv_layer"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Apply Max Pooling")]),t._v("\nconv_layer "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("max_pool"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    conv_layer"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    ksize"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    strides"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    padding"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token string"}},[t._v("'SAME'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),n("p",[t._v("The ksize and strides parameters are structured as 4-element lists, with each element corresponding to a dimension of the input tensor ([batch, height, width, channels]). For both ksize and strides, the batch and channel dimensions are typically set to 1.")]),n("h4",{attrs:{id:"典型的cnn结构"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#典型的cnn结构","aria-hidden":"true"}},[t._v("#")]),t._v(" 典型的CNN结构")]),n("p",[n("img",{attrs:{src:s(195),alt:""}})]),n("h4",{attrs:{id:"池化层的输出"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#池化层的输出","aria-hidden":"true"}},[t._v("#")]),t._v(" 池化层的输出")]),n("p",[t._v("池化层不改变输出的深度，它单独应用到每一个深度切片上。")]),n("p",[t._v("假定：H = height, W = width, D = depth")]),n("ul",[n("li",[t._v("We have an input of shape 4x4x5 (HxWxD)")]),n("li",[t._v("Filter of shape 2x2 (HxW)")]),n("li",[t._v("A stride of 2 for both the height and width (S)")])]),n("p",[n("img",{attrs:{src:s(194),alt:""}})]),n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("new_height = (input_height - filter_height)/S + 1\nnew_width = (input_width - filter_width)/S + 1\n")])]),n("p",[t._v("输出层的尺寸为2x2x5")]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{attrs:{class:"token builtin"}},[t._v("input")]),t._v(" "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("placeholder"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("float32"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token boolean"}},[t._v("None")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("4")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("4")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("5")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfilter_shape "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nstrides "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\npadding "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token string"}},[t._v("'VALID'")]),t._v("\npool "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("max_pool"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token builtin"}},[t._v("input")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" filter_shape"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" strides"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" padding"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),n("p",[t._v("输出结果为  [1, 2, 2, 5], 即使padding方式修改为： 'SAME'")]),n("h3",{attrs:{id:"_5-1x1卷积"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_5-1x1卷积","aria-hidden":"true"}},[t._v("#")]),t._v(" 5. 1x1卷积")]),n("p",[n("img",{attrs:{src:s(193),alt:""}})]),n("p",[t._v("1x1卷积可以增加输出的深度，且运算非常简单")]),n("h3",{attrs:{id:"_6-inception-模块"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_6-inception-模块","aria-hidden":"true"}},[t._v("#")]),t._v(" 6. inception 模块")]),n("p",[n("img",{attrs:{src:s(192),alt:""}})]),n("h2",{attrs:{id:"_3-使用tensorflow构建cnn"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_3-使用tensorflow构建cnn","aria-hidden":"true"}},[t._v("#")]),t._v(" 3. 使用Tensorflow构建CNN")]),n("p",[t._v("TensorFlow 提供了 "),n("code",[t._v("tf.nn.conv2d()")]),t._v("和"),n("code",[t._v("tf.nn.bias_add()")]),t._v(" 两个函数来构建CNN")]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{attrs:{class:"token comment"}},[t._v("# Output depth")]),t._v("\nk_output "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("64")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Image Properties")]),t._v("\nimage_width "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("10")]),t._v("\nimage_height "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("10")]),t._v("\ncolor_channels "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("3")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Convolution filter")]),t._v("\nfilter_size_width "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("5")]),t._v("\nfilter_size_height "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("5")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Input/Image")]),t._v("\n"),n("span",{attrs:{class:"token builtin"}},[t._v("input")]),t._v(" "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("placeholder"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("float32"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    shape"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token boolean"}},[t._v("None")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" image_height"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" image_width"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" color_channels"),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Weight and bias")]),t._v("\nweight "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("truncated_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    "),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("filter_size_height"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" filter_size_width"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" color_channels"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" k_output"),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nbias "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("k_output"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Apply Convolution")]),t._v("\nconv_layer "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv2d"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token builtin"}},[t._v("input")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weight"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" strides"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" padding"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token string"}},[t._v("'SAME'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Add bias")]),t._v("\nconv_layer "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bias_add"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("conv_layer"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" bias"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Apply activation function")]),t._v("\nconv_layer "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("relu"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("conv_layer"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),n("p",[t._v("使用 "),n("code",[t._v("tf.nn.conv2d()")]),t._v(" 来构建一个卷积层，步长是**"),n("code",[t._v("[1, 2, 2, 1]")]),n("strong",[t._v("，分别对应")]),n("code",[t._v("[batch, input_height, input_width, input_channels]")]),t._v("**， 其中，batch和input_channels我们一般就设置为1。")]),n("h3",{attrs:{id:"例程"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#例程","aria-hidden":"true"}},[t._v("#")]),t._v(" 例程")]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("examples"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tutorials"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mnist "),n("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" input_data\nmnist "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" input_data"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_data_sets"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token string"}},[t._v('"."')]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" one_hot"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token boolean"}},[t._v("True")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" reshape"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token boolean"}},[t._v("False")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" tensorflow "),n("span",{attrs:{class:"token keyword"}},[t._v("as")]),t._v(" tf\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Parameters")]),t._v("\nlearning_rate "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("0.00001")]),t._v("\nepochs "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("10")]),t._v("\nbatch_size "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("128")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Number of samples to calculate validation and accuracy")]),t._v("\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Decrease this if you're running out of memory to calculate accuracy")]),t._v("\ntest_valid_size "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("256")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Network Parameters")]),t._v("\nn_classes "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("10")]),t._v("  "),n("span",{attrs:{class:"token comment"}},[t._v("# MNIST total classes (0-9 digits)")]),t._v("\ndropout "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("0.75")]),t._v("  "),n("span",{attrs:{class:"token comment"}},[t._v("# Dropout, probability to keep units")]),t._v("\n")])]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{attrs:{class:"token comment"}},[t._v("# Store layers weight & bias")]),t._v("\nweights "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{attrs:{class:"token string"}},[t._v("'wc1'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("5")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("5")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("32")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{attrs:{class:"token string"}},[t._v("'wc2'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("5")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("5")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("32")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("64")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{attrs:{class:"token string"}},[t._v("'wd1'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("7")]),n("span",{attrs:{class:"token operator"}},[t._v("*")]),n("span",{attrs:{class:"token number"}},[t._v("7")]),n("span",{attrs:{class:"token operator"}},[t._v("*")]),n("span",{attrs:{class:"token number"}},[t._v("64")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1024")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{attrs:{class:"token string"}},[t._v("'out'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("1024")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_classes"),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\nbiases "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{attrs:{class:"token string"}},[t._v("'bc1'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("32")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{attrs:{class:"token string"}},[t._v("'bc2'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("64")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{attrs:{class:"token string"}},[t._v("'bd1'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("1024")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{attrs:{class:"token string"}},[t._v("'out'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("n_classes"),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])]),n("h4",{attrs:{id:"卷积"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#卷积","aria-hidden":"true"}},[t._v("#")]),n("a",{attrs:{href:"https://en.wikipedia.org/wiki/Convolution",target:"_blank",rel:"noopener noreferrer"}},[t._v("卷积")])]),n("p",[n("img",{attrs:{src:"http://deeplearning.stanford.edu/wiki/images/6/6c/Convolution_schematic.gif",alt:""}})]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{attrs:{class:"token function"}},[t._v("conv2d")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token builtin"}},[t._v("input")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{attrs:{class:"token comment"}},[t._v("# Filter (weights and bias)")]),t._v("\n    F_W "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("truncated_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("3")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    F_b "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token number"}},[t._v("3")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    strides "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    padding "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token string"}},[t._v("'VALID'")]),t._v("\n    "),n("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv2d"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token builtin"}},[t._v("input")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" F_W"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" strides"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" padding"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{attrs:{class:"token operator"}},[t._v("+")]),t._v(" F_b\n")])]),n("h4",{attrs:{id:"最大池化"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#最大池化","aria-hidden":"true"}},[t._v("#")]),n("a",{attrs:{href:"https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer",target:"_blank",rel:"noopener noreferrer"}},[t._v("最大池化")])]),n("p",[n("img",{attrs:{src:s(191),alt:""}})]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{attrs:{class:"token function"}},[t._v("maxpool2d")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" k"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("max_pool"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        x"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        ksize"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" k"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" k"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        strides"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" k"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" k"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        padding"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token string"}},[t._v("'SAME'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),n("h4",{attrs:{id:"模型结构"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#模型结构","aria-hidden":"true"}},[t._v("#")]),t._v(" 模型结构")]),n("p",[n("img",{attrs:{src:s(190),alt:""}})]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{attrs:{class:"token function"}},[t._v("conv_net")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weights"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" biases"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dropout"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{attrs:{class:"token comment"}},[t._v("# Layer 1 - 28*28*1 to 14*14*32")]),t._v("\n    conv1 "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" conv2d"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weights"),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token string"}},[t._v("'wc1'")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" biases"),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token string"}},[t._v("'bc1'")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    conv1 "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" maxpool2d"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("conv1"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" k"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{attrs:{class:"token comment"}},[t._v("# Layer 2 - 14*14*32 to 7*7*64")]),t._v("\n    conv2 "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" conv2d"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("conv1"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weights"),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token string"}},[t._v("'wc2'")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" biases"),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token string"}},[t._v("'bc2'")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    conv2 "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" maxpool2d"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("conv2"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" k"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{attrs:{class:"token comment"}},[t._v("# Fully connected layer - 7*7*64 to 1024")]),t._v("\n    fc1 "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("conv2"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token operator"}},[t._v("-")]),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weights"),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token string"}},[t._v("'wd1'")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_shape"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("as_list"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("0")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    fc1 "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("matmul"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fc1"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weights"),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token string"}},[t._v("'wd1'")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" biases"),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token string"}},[t._v("'bd1'")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    fc1 "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("relu"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fc1"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    fc1 "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dropout"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fc1"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dropout"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{attrs:{class:"token comment"}},[t._v("# Output Layer - class prediction - 1024 to 10")]),t._v("\n    out "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("matmul"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fc1"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weights"),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token string"}},[t._v("'out'")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" biases"),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token string"}},[t._v("'out'")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" out\n")])]),n("h4",{attrs:{id:"session"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#session","aria-hidden":"true"}},[t._v("#")]),t._v(" Session")]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{attrs:{class:"token comment"}},[t._v("# tf Graph input")]),t._v("\nx "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("placeholder"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("float32"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token boolean"}},[t._v("None")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("28")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("28")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ny "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("placeholder"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("float32"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token boolean"}},[t._v("None")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_classes"),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nkeep_prob "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("placeholder"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("float32"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Model")]),t._v("\nlogits "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" conv_net"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weights"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" biases"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" keep_prob"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Define loss and optimizer")]),t._v("\ncost "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reduce_mean"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\\\n    tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("softmax_cross_entropy_with_logits"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("logits"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("logits"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("y"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\noptimizer "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("GradientDescentOptimizer"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("learning_rate"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("learning_rate"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\\\n    "),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("minimize"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cost"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Accuracy")]),t._v("\ncorrect_pred "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("equal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("argmax"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("logits"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("argmax"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("y"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\naccuracy "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reduce_mean"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cast"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("correct_pred"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("float32"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Initializing the variables")]),t._v("\ninit "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v(" global_variables_initializer"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Launch the graph")]),t._v("\n"),n("span",{attrs:{class:"token keyword"}},[t._v("with")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Session"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{attrs:{class:"token keyword"}},[t._v("as")]),t._v(" sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("init"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" epoch "),n("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{attrs:{class:"token builtin"}},[t._v("range")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("epochs"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" batch "),n("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{attrs:{class:"token builtin"}},[t._v("range")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mnist"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("num_examples"),n("span",{attrs:{class:"token operator"}},[t._v("//")]),t._v("batch_size"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            batch_x"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" batch_y "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" mnist"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("next_batch"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("batch_size"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("optimizer"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" feed_dict"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                x"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" batch_x"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                y"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" batch_y"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                keep_prob"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" dropout"),n("span",{attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),n("span",{attrs:{class:"token comment"}},[t._v("# Calculate batch loss and accuracy")]),t._v("\n            loss "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cost"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" feed_dict"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                x"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" batch_x"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                y"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" batch_y"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                keep_prob"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            valid_acc "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("accuracy"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" feed_dict"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                x"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" mnist"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("validation"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("images"),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("test_valid_size"),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                y"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" mnist"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("validation"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("labels"),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("test_valid_size"),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                keep_prob"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),n("span",{attrs:{class:"token keyword"}},[t._v("print")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token string"}},[t._v("'Epoch {:>2}, Batch {:>3} -'")]),t._v("\n                  "),n("span",{attrs:{class:"token string"}},[t._v("'Loss: {:>10.4f} Validation Accuracy: {:.6f}'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{attrs:{class:"token builtin"}},[t._v("format")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n                epoch "),n("span",{attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                batch "),n("span",{attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                loss"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                valid_acc"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{attrs:{class:"token comment"}},[t._v("# Calculate Test Accuracy")]),t._v("\n    test_acc "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("accuracy"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" feed_dict"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        x"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" mnist"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("test"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("images"),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("test_valid_size"),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        y"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" mnist"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("test"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("labels"),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("test_valid_size"),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        keep_prob"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{attrs:{class:"token keyword"}},[t._v("print")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token string"}},[t._v("'Testing Accuracy: {}'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{attrs:{class:"token builtin"}},[t._v("format")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_acc"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),n("h2",{attrs:{id:"_4-实现lenet"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_4-实现lenet","aria-hidden":"true"}},[t._v("#")]),t._v(" 4. 实现LeNet")]),n("h3",{attrs:{id:"预处理"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#预处理","aria-hidden":"true"}},[t._v("#")]),t._v(" 预处理")]),n("p",[t._v("An MNIST image is initially 784 features (1D). If the data is not normalized from [0, 255] to [0, 1], normalize it. We reshape this to (28, 28, 1) (3D), and pad the image with 0s such that the height and width are 32 (centers digit further). Thus, the input shape going into the first convolutional layer is 32x32x1.")]),n("h4",{attrs:{id:"网络模型规格"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#网络模型规格","aria-hidden":"true"}},[t._v("#")]),t._v(" 网络模型规格")]),n("table",[n("thead",[n("tr",[n("th",{staticStyle:{"text-align":"center"}},[t._v("分层")]),n("th",{staticStyle:{"text-align":"center"}},[t._v("输出尺寸")]),n("th",{staticStyle:{"text-align":"center"}},[t._v("备注")])])]),n("tbody",[n("tr",[n("td",{staticStyle:{"text-align":"center"}},[t._v("卷积层-1")]),n("td",{staticStyle:{"text-align":"center"}},[t._v("28x28x6")]),n("td",{staticStyle:{"text-align":"center"}})]),n("tr",[n("td",{staticStyle:{"text-align":"center"}},[t._v("激活层-1")]),n("td",{staticStyle:{"text-align":"center"}},[t._v("激活函数")]),n("td",{staticStyle:{"text-align":"center"}})]),n("tr",[n("td",{staticStyle:{"text-align":"center"}},[t._v("池化层-1")]),n("td",{staticStyle:{"text-align":"center"}},[t._v("14x14x6")]),n("td",{staticStyle:{"text-align":"center"}})]),n("tr",[n("td",{staticStyle:{"text-align":"center"}},[t._v("卷积层-2")]),n("td",{staticStyle:{"text-align":"center"}},[t._v("10x10x16")]),n("td",{staticStyle:{"text-align":"center"}})]),n("tr",[n("td",{staticStyle:{"text-align":"center"}},[t._v("激活层-2")]),n("td",{staticStyle:{"text-align":"center"}},[t._v("激活函数")]),n("td",{staticStyle:{"text-align":"center"}})]),n("tr",[n("td",{staticStyle:{"text-align":"center"}},[t._v("池化层-2")]),n("td",{staticStyle:{"text-align":"center"}},[t._v("5x5x16")]),n("td",{staticStyle:{"text-align":"center"}})]),n("tr",[n("td",{staticStyle:{"text-align":"center"}},[t._v("扁平层")]),n("td",{staticStyle:{"text-align":"center"}},[t._v("转换为1维数据")]),n("td",{staticStyle:{"text-align":"center"}},[n("code",[t._v("tf.contrib.layers.flatten")])])]),n("tr",[n("td",{staticStyle:{"text-align":"center"}},[t._v("全连接层-1")]),n("td",{staticStyle:{"text-align":"center"}},[t._v("120")]),n("td",{staticStyle:{"text-align":"center"}})]),n("tr",[n("td",{staticStyle:{"text-align":"center"}},[t._v("激活层-3")]),n("td",{staticStyle:{"text-align":"center"}},[t._v("激活函数")]),n("td",{staticStyle:{"text-align":"center"}})]),n("tr",[n("td",{staticStyle:{"text-align":"center"}},[t._v("全连接层-2")]),n("td",{staticStyle:{"text-align":"center"}},[t._v("84")]),n("td",{staticStyle:{"text-align":"center"}})]),n("tr",[n("td",{staticStyle:{"text-align":"center"}},[t._v("激活层-4")]),n("td",{staticStyle:{"text-align":"center"}},[t._v("激活函数")]),n("td",{staticStyle:{"text-align":"center"}})]),n("tr",[n("td",{staticStyle:{"text-align":"center"}},[t._v("全连接层-3")]),n("td",{staticStyle:{"text-align":"center"}},[t._v("10")]),n("td",{staticStyle:{"text-align":"center"}})])])]),n("div",{staticClass:"tip custom-block"},[n("p",{staticClass:"custom-block-title"},[t._v("扩展阅读")]),n("ol",[n("li",[n("a",{attrs:{href:"https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/",target:"_blank",rel:"noopener noreferrer"}},[t._v("An Intuitive Explanation of Convolutional Neural Networks")])]),n("li",[n("a",{attrs:{href:"https://en.wikipedia.org/wiki/Artificial_neuron",target:"_blank",rel:"noopener noreferrer"}},[t._v("Artificial neuron")])]),n("li",[n("a",{attrs:{href:"http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution",target:"_blank",rel:"noopener noreferrer"}},[t._v("Feature extraction using convolution")])]),n("li",[n("a",{attrs:{href:"http://www.matthewzeiler.com/wp-content/uploads/2017/07/eccv2014.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Visualizing and Understanding Convolutional Networks")])]),n("li",[n("a",{attrs:{href:"https://en.wikipedia.org/wiki/Convolutional_neural_network",target:"_blank",rel:"noopener noreferrer"}},[t._v("wiki/Convolutional_neural_network")])]),n("li",[n("a",{attrs:{href:"https://www.deeplearningbook.org/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Deep Learning")])]),n("li",[n("a",{attrs:{href:"http://cs231n.github.io/",target:"_blank",rel:"noopener noreferrer"}},[t._v("CS231n Convolutional Neural Networks for Visual Recognition")])])])])])}],o=s(0),e=Object(o.a)({},function(){this.$createElement;this._self._c;return this._m(0)},n,!1,null,null,null);a.default=e.exports}}]);