(window.webpackJsonp=window.webpackJsonp||[]).push([[15],{173:function(t,s,a){t.exports=a.p+"assets/img/15380662426023.4fbad573.jpg"},174:function(t,s,a){t.exports=a.p+"assets/img/15380659683416.5bacf491.jpg"},175:function(t,s,a){t.exports=a.p+"assets/img/15380658421658.c8bf5431.jpg"},176:function(t,s,a){t.exports=a.p+"assets/img/15380654677950.ab0a54ea.jpg"},177:function(t,s,a){t.exports=a.p+"assets/img/15380652743380.68048b89.jpg"},178:function(t,s,a){t.exports=a.p+"assets/img/15380633034071.8e36902e.jpg"},179:function(t,s,a){t.exports=a.p+"assets/img/15380623134890.bd625fc5.jpg"},180:function(t,s,a){t.exports=a.p+"assets/img/15380077486710.3c5f1534.jpg"},181:function(t,s,a){t.exports=a.p+"assets/img/15380073455603.935647aa.jpg"},182:function(t,s,a){t.exports=a.p+"assets/img/15380072209022.8eca863f.jpg"},183:function(t,s,a){t.exports=a.p+"assets/img/15380065956269.8a2c4456.jpg"},184:function(t,s,a){t.exports=a.p+"assets/img/15380065684643.7ea292a2.jpg"},354:function(t,s,a){"use strict";a.r(s);var n=[function(){var t=this,s=t.$createElement,n=t._self._c||s;return n("div",{staticClass:"content"},[n("h1",{attrs:{id:"深度神经网络"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#深度神经网络","aria-hidden":"true"}},[t._v("#")]),t._v(" 深度神经网络")]),n("h2",{attrs:{id:"_1-深度神经网络简介"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_1-深度神经网络简介","aria-hidden":"true"}},[t._v("#")]),t._v(" 1. 深度神经网络简介")]),n("h3",{attrs:{id:"_1-线性模型复杂度"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_1-线性模型复杂度","aria-hidden":"true"}},[t._v("#")]),t._v(" 1. 线性模型复杂度")]),n("p",[n("img",{attrs:{src:a(184),alt:""}})]),n("p",[t._v("线性模型中，可调参数个数=w维数展开+b维数展开，即当我们有N个输入，K个输出时，我们的可调参数个数为(N+1)K")]),n("p",[n("img",{attrs:{src:a(183),alt:""}})]),n("ul",[n("li",[t._v("线性模型比较稳定，因为数据是线性叠加的，微小的输入不会引起结果的剧烈变化")]),n("li",[t._v("因为模型是线性的，所以能够表示的关系是有限的，只能表示线性关系（输入是相加而非相乘）")]),n("li",[t._v("线性函数的导数是常量")]),n("li",[t._v("我们希望模型是非线性的，但是参数存放在线性的方程中，因此我们必须添加非线性成分")]),n("li",[t._v("我们需要大量的可调参数，而不是固定的(N+1)K个")])]),n("h3",{attrs:{id:"_2-rectified-linear-units（relus）"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_2-rectified-linear-units（relus）","aria-hidden":"true"}},[t._v("#")]),t._v(" 2. Rectified Linear Units（ReLUs）")]),n("p",[n("img",{attrs:{src:a(182),alt:""}})]),n("p",[n("img",{attrs:{src:a(181),alt:""}})]),n("p",[t._v("为了解决上一节提出的问题，我们引入"),n("strong",[t._v("ReLU")]),t._v("函数，将其插入到矩阵中。以往，我们在构造多层的神经网络时，不同层的W是相乘的，然后得到一个W作为整体参与到结果的运算中，现在我们需要在Wi相乘的过程中插入"),n("strong",[t._v("ReLUs")]),t._v("，这样模型就变成非线性模型来，同时我们可以调节隐藏层"),n("strong",[t._v("ReLUs")]),t._v("的数量以达到增加参数的目的。")]),n("div",{staticClass:"tip custom-block"},[n("p",{staticClass:"custom-block-title"},[t._v("注意")]),n("p",[n("strong",[t._v("ReLUs")]),t._v("也是一种"),n("a",{attrs:{href:"https://zh.wikipedia.org/wiki/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0",target:"_blank",rel:"noopener noreferrer"}},[t._v("激活函数")]),t._v("，目前我们已经接触到的激活函数有：sigmoid，softmax，ReLUs")])]),n("p",[t._v("在tensorflow中，我们使用"),n("code",[t._v("tf.nn.relu()")]),t._v("来调用relu函数")]),n("h3",{attrs:{id:"_3-multilayer-neural-networks"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_3-multilayer-neural-networks","aria-hidden":"true"}},[t._v("#")]),t._v(" 3. Multilayer Neural Networks")]),n("p",[t._v("在网络中添加隐藏层可以让模型变得更复杂，同时，在隐藏层中添加非线性的激活函数，可以让模型变成非线性的。")]),n("p",[t._v("假定我们构造一个2层的神经网络：\n"),n("img",{attrs:{src:a(180),alt:""}})]),n("ul",[n("li",[t._v("第一层保护来一组权重和偏差，我们将X输入到这一层，并传入到激活函数"),n("strong",[t._v("ReLUs")]),t._v("中，输出的结果会输入到下一层（隐藏层）")]),n("li",[t._v("隐藏层将结果和本层的权重、偏差进行计算，得到输出层结果y，然后使用softmax函数将其转换为概率")])]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{attrs:{class:"token comment"}},[t._v("# Hidden Layer with ReLU activation function")]),t._v("\nhidden_layer "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("matmul"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("features"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hidden_weights"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hidden_biases"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nhidden_layer "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("relu"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hidden_layer"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\noutput "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("matmul"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hidden_layer"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" output_weights"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" output_biases"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),n("h3",{attrs:{id:"_4-链式法则及反向传播"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_4-链式法则及反向传播","aria-hidden":"true"}},[t._v("#")]),t._v(" 4. 链式法则及反向传播")]),n("p",[t._v("这里我们同样会利用导数的链式法则，通过求解各部分导数，然后将其相乘，得到总体的导数。可以参考"),n("a",{attrs:{href:"http://localhost:8080/udacity-self-driving-car-notes/posts/self-driving-car/deeplearning.html#%E5%AF%BC%E6%95%B0%E8%AE%A1%E7%AE%97%E4%B9%8B%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99",target:"_blank",rel:"noopener noreferrer"}},[t._v("链式法则")])]),n("p",[n("img",{attrs:{src:a(179),alt:""}})]),n("h2",{attrs:{id:"_2-基于tensorflow的深度神经网络"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_2-基于tensorflow的深度神经网络","aria-hidden":"true"}},[t._v("#")]),t._v(" 2. 基于tensorflow的深度神经网络")]),n("h3",{attrs:{id:"_1-例程"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_1-例程","aria-hidden":"true"}},[t._v("#")]),t._v(" 1.  例程")]),n("h4",{attrs:{id:"学习参数"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#学习参数","aria-hidden":"true"}},[t._v("#")]),t._v(" 学习参数")]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" tensorflow "),n("span",{attrs:{class:"token keyword"}},[t._v("as")]),t._v(" tf\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Parameters")]),t._v("\nlearning_rate "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("0.001")]),t._v("\ntraining_epochs "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("20")]),t._v("\nbatch_size "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("128")]),t._v("  "),n("span",{attrs:{class:"token comment"}},[t._v("# Decrease batch size if you don't have enough memory")]),t._v("\ndisplay_step "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),t._v("\n\nn_input "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("784")]),t._v("  "),n("span",{attrs:{class:"token comment"}},[t._v("# MNIST data input (img shape: 28*28)")]),t._v("\nn_classes "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("10")]),t._v("  "),n("span",{attrs:{class:"token comment"}},[t._v("# MNIST total classes (0-9 digits)")]),t._v("\n")])]),n("h4",{attrs:{id:"隐藏层参数"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#隐藏层参数","aria-hidden":"true"}},[t._v("#")]),t._v(" 隐藏层参数")]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("n_hidden_layer "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("256")]),t._v(" "),n("span",{attrs:{class:"token comment"}},[t._v("# layer number of features")]),t._v("\n")])]),n("h4",{attrs:{id:"权重和偏差"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#权重和偏差","aria-hidden":"true"}},[t._v("#")]),t._v(" 权重和偏差")]),n("p",[t._v("这里我们为不同的层创建不同的权重和偏差")]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{attrs:{class:"token comment"}},[t._v("# Store layers weight & bias")]),t._v("\nweights "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{attrs:{class:"token string"}},[t._v("'hidden_layer'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("n_input"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_hidden_layer"),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{attrs:{class:"token string"}},[t._v("'out'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("n_hidden_layer"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_classes"),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\nbiases "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{attrs:{class:"token string"}},[t._v("'hidden_layer'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("n_hidden_layer"),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{attrs:{class:"token string"}},[t._v("'out'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("n_classes"),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])]),n("h4",{attrs:{id:"输入"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#输入","aria-hidden":"true"}},[t._v("#")]),t._v(" 输入")]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{attrs:{class:"token comment"}},[t._v("# tf Graph input")]),t._v("\nx "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("placeholder"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token string"}},[t._v('"float"')]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token boolean"}},[t._v("None")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("28")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("28")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ny "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("placeholder"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token string"}},[t._v('"float"')]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token boolean"}},[t._v("None")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_classes"),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nx_flat "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token operator"}},[t._v("-")]),n("span",{attrs:{class:"token number"}},[t._v("1")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" n_input"),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),n("h4",{attrs:{id:"多层感知元"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#多层感知元","aria-hidden":"true"}},[t._v("#")]),t._v(" 多层感知元")]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{attrs:{class:"token comment"}},[t._v("# Hidden layer with RELU activation")]),t._v("\nlayer_1 "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("matmul"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x_flat"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weights"),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token string"}},[t._v("'hidden_layer'")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\\\n    biases"),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token string"}},[t._v("'hidden_layer'")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlayer_1 "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("relu"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("layer_1"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Output layer with linear activation")]),t._v("\nlogits "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("matmul"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("layer_1"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weights"),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token string"}},[t._v("'out'")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" biases"),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token string"}},[t._v("'out'")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),n("h4",{attrs:{id:"优化器"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#优化器","aria-hidden":"true"}},[t._v("#")]),t._v(" 优化器")]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{attrs:{class:"token comment"}},[t._v("# Define loss and optimizer")]),t._v("\ncost "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reduce_mean"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\\\n    tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("softmax_cross_entropy_with_logits"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("logits"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("logits"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("y"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\noptimizer "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("GradientDescentOptimizer"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("learning_rate"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("learning_rate"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\\\n    "),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("minimize"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cost"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),n("h4",{attrs:{id:"会话"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#会话","aria-hidden":"true"}},[t._v("#")]),t._v(" 会话")]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{attrs:{class:"token comment"}},[t._v("# Initializing the variables")]),t._v("\ninit "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("global_variables_initializer"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Launch the graph")]),t._v("\n"),n("span",{attrs:{class:"token keyword"}},[t._v("with")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Session"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{attrs:{class:"token keyword"}},[t._v("as")]),t._v(" sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("init"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{attrs:{class:"token comment"}},[t._v("# Training cycle")]),t._v("\n    "),n("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" epoch "),n("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{attrs:{class:"token builtin"}},[t._v("range")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("training_epochs"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        total_batch "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token builtin"}},[t._v("int")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mnist"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("num_examples"),n("span",{attrs:{class:"token operator"}},[t._v("/")]),t._v("batch_size"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{attrs:{class:"token comment"}},[t._v("# Loop over all batches")]),t._v("\n        "),n("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),n("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{attrs:{class:"token builtin"}},[t._v("range")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("total_batch"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            batch_x"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" batch_y "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" mnist"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("next_batch"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("batch_size"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),n("span",{attrs:{class:"token comment"}},[t._v("# Run optimization op (backprop) and cost op (to get loss value)")]),t._v("\n            sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("optimizer"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" feed_dict"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("x"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" batch_x"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" batch_y"),n("span",{attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),n("h3",{attrs:{id:"_2-训练神经网络"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_2-训练神经网络","aria-hidden":"true"}},[t._v("#")]),t._v(" 2. 训练神经网络")]),n("p",[t._v("我们可以有两种方式来扩展我们的神经网络")]),n("ul",[n("li",[t._v("更广：增加隐藏层H的数量，但是参数过多会难以训练")]),n("li",[t._v("更深：增加多层神经网络")])]),n("p",[t._v("更深的方向是比较好的思路，一方面参数比较少，另一方面它会呈现出明显的结构特征，每一层可以学到不同的信息。学习速率也更快。\n"),n("img",{attrs:{src:a(178),alt:""}})]),n("p",[t._v("学习完成之后，我们自然希望将结果存储下来，此时可以使用："),n("code",[t._v("tf.train.Saver.")])]),n("h3",{attrs:{id:"_3-储存变量和模型"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_3-储存变量和模型","aria-hidden":"true"}},[t._v("#")]),t._v(" 3. 储存变量和模型")]),n("h4",{attrs:{id:"储存变量"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#储存变量","aria-hidden":"true"}},[t._v("#")]),t._v(" 储存变量")]),n("p",[t._v("使用"),n("code",[t._v("tf.train.Saver.save()")]),t._v(" 函数储存数据到**.ckpt格式文件中**. (checkpoint)")]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" tensorflow "),n("span",{attrs:{class:"token keyword"}},[t._v("as")]),t._v(" tf\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# The file path to save the data")]),t._v("\nsave_file "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token string"}},[t._v("'./model.ckpt'")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Two Tensor Variables: weights and bias")]),t._v("\nweights "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("truncated_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("3")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nbias "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("truncated_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("3")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Class used to save and/or restore Tensor Variables")]),t._v("\nsaver "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Saver"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token keyword"}},[t._v("with")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Session"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{attrs:{class:"token keyword"}},[t._v("as")]),t._v(" sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{attrs:{class:"token comment"}},[t._v("# Initialize all the Variables")]),t._v("\n    sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("global_variables_initializer"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{attrs:{class:"token comment"}},[t._v("# Show the values of weights and bias")]),t._v("\n    "),n("span",{attrs:{class:"token keyword"}},[t._v("print")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token string"}},[t._v("'Weights:'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{attrs:{class:"token keyword"}},[t._v("print")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("weights"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{attrs:{class:"token keyword"}},[t._v("print")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token string"}},[t._v("'Bias:'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{attrs:{class:"token keyword"}},[t._v("print")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bias"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{attrs:{class:"token comment"}},[t._v("# Save the model")]),t._v("\n    saver"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" save_file"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),n("h4",{attrs:{id:"加载数据"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#加载数据","aria-hidden":"true"}},[t._v("#")]),t._v(" 加载数据")]),n("p",[t._v("因为"),n("code",[t._v("tf.train.Saver.restore()")]),t._v("函数在载入时会设置数据，不必要再调用"),n("code",[t._v("tf.global_variables_initializer().")])]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{attrs:{class:"token comment"}},[t._v("# Remove the previous weights and bias")]),t._v("\ntf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reset_default_graph"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Two Variables: weights and bias")]),t._v("\nweights "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("truncated_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("3")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nbias "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("truncated_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("3")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Class used to save and/or restore Tensor Variables")]),t._v("\nsaver "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Saver"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token keyword"}},[t._v("with")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Session"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{attrs:{class:"token keyword"}},[t._v("as")]),t._v(" sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{attrs:{class:"token comment"}},[t._v("# Load the weights and bias")]),t._v("\n    saver"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("restore"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" save_file"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{attrs:{class:"token comment"}},[t._v("# Show the values of weights and bias")]),t._v("\n    "),n("span",{attrs:{class:"token keyword"}},[t._v("print")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token string"}},[t._v("'Weight:'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{attrs:{class:"token keyword"}},[t._v("print")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("weights"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{attrs:{class:"token keyword"}},[t._v("print")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token string"}},[t._v("'Bias:'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{attrs:{class:"token keyword"}},[t._v("print")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bias"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),n("h4",{attrs:{id:"储存完整模型"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#储存完整模型","aria-hidden":"true"}},[t._v("#")]),t._v(" 储存完整模型")]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" math\n\nsave_file "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token string"}},[t._v("'./train_model.ckpt'")]),t._v("\nbatch_size "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("128")]),t._v("\nn_epochs "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("100")]),t._v("\n\nsaver "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Saver"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Launch the graph")]),t._v("\n"),n("span",{attrs:{class:"token keyword"}},[t._v("with")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Session"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{attrs:{class:"token keyword"}},[t._v("as")]),t._v(" sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("global_variables_initializer"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{attrs:{class:"token comment"}},[t._v("# Training cycle")]),t._v("\n    "),n("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" epoch "),n("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{attrs:{class:"token builtin"}},[t._v("range")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n_epochs"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        total_batch "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" math"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ceil"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mnist"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("num_examples "),n("span",{attrs:{class:"token operator"}},[t._v("/")]),t._v(" batch_size"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),n("span",{attrs:{class:"token comment"}},[t._v("# Loop over all batches")]),t._v("\n        "),n("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),n("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{attrs:{class:"token builtin"}},[t._v("range")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("total_batch"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            batch_features"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" batch_labels "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" mnist"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("next_batch"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("batch_size"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n                optimizer"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                feed_dict"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("features"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" batch_features"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" batch_labels"),n("span",{attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),n("span",{attrs:{class:"token comment"}},[t._v("# Print status for every 10 epochs")]),t._v("\n        "),n("span",{attrs:{class:"token keyword"}},[t._v("if")]),t._v(" epoch "),n("span",{attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("10")]),t._v(" "),n("span",{attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("0")]),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            valid_accuracy "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n                accuracy"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                feed_dict"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                    features"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" mnist"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("validation"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("images"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    labels"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" mnist"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("validation"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("labels"),n("span",{attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),n("span",{attrs:{class:"token keyword"}},[t._v("print")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token string"}},[t._v("'Epoch {:<3} - Validation Accuracy: {}'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{attrs:{class:"token builtin"}},[t._v("format")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n                epoch"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                valid_accuracy"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{attrs:{class:"token comment"}},[t._v("# Save the model")]),t._v("\n    saver"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" save_file"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{attrs:{class:"token keyword"}},[t._v("print")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token string"}},[t._v("'Trained Model Saved.'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),n("h4",{attrs:{id:"加载模型"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#加载模型","aria-hidden":"true"}},[t._v("#")]),t._v(" 加载模型")]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("saver "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Saver"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Launch the graph")]),t._v("\n"),n("span",{attrs:{class:"token keyword"}},[t._v("with")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Session"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{attrs:{class:"token keyword"}},[t._v("as")]),t._v(" sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    saver"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("restore"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" save_file"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    test_accuracy "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        accuracy"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        feed_dict"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("features"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" mnist"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("test"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("images"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" mnist"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("test"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("labels"),n("span",{attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token keyword"}},[t._v("print")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token string"}},[t._v("'Test Accuracy: {}'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{attrs:{class:"token builtin"}},[t._v("format")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_accuracy"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),n("h3",{attrs:{id:"_4-加载参数到新的模型"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_4-加载参数到新的模型","aria-hidden":"true"}},[t._v("#")]),t._v(" 4. 加载参数到新的模型")]),n("p",[t._v("TensorFlow 使用"),n("strong",[t._v("name")]),t._v("参数来标记张量和运算，如果没有设置"),n("strong",[t._v("name")]),t._v("，则tensorflow会自动设置为"),n("code",[t._v("<Type>_<number>")]),t._v("，根据变量声明的顺序和类型来命名。因此，如果将一个模型的参数导入另一个，可能会因为顺序等原因，造成错误的自动赋值，因此我们需要手工的指定。")]),n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" tensorflow "),n("span",{attrs:{class:"token keyword"}},[t._v("as")]),t._v(" tf\n\ntf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reset_default_graph"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nsave_file "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{attrs:{class:"token string"}},[t._v("'model.ckpt'")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Two Tensor Variables: weights and bias")]),t._v("\nweights "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("truncated_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("3")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" name"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token string"}},[t._v("'weights_0'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nbias "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("truncated_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("3")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" name"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token string"}},[t._v("'bias_0'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nsaver "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Saver"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Print the name of Weights and Bias")]),t._v("\n"),n("span",{attrs:{class:"token keyword"}},[t._v("print")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token string"}},[t._v("'Save Weights: {}'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{attrs:{class:"token builtin"}},[t._v("format")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("weights"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("name"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{attrs:{class:"token keyword"}},[t._v("print")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token string"}},[t._v("'Save Bias: {}'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{attrs:{class:"token builtin"}},[t._v("format")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bias"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("name"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token keyword"}},[t._v("with")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Session"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{attrs:{class:"token keyword"}},[t._v("as")]),t._v(" sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("global_variables_initializer"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    saver"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" save_file"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Remove the previous weights and bias")]),t._v("\ntf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reset_default_graph"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Two Variables: weights and bias")]),t._v("\nbias "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("truncated_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("3")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" name"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token string"}},[t._v("'bias_0'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nweights "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("truncated_normal"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{attrs:{class:"token number"}},[t._v("2")]),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{attrs:{class:"token number"}},[t._v("3")]),n("span",{attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("name"),n("span",{attrs:{class:"token operator"}},[t._v("=")]),n("span",{attrs:{class:"token string"}},[t._v("'weights_0'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nsaver "),n("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Saver"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token comment"}},[t._v("# Print the name of Weights and Bias")]),t._v("\n"),n("span",{attrs:{class:"token keyword"}},[t._v("print")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token string"}},[t._v("'Load Weights: {}'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{attrs:{class:"token builtin"}},[t._v("format")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("weights"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("name"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{attrs:{class:"token keyword"}},[t._v("print")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token string"}},[t._v("'Load Bias: {}'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{attrs:{class:"token builtin"}},[t._v("format")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bias"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("name"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token keyword"}},[t._v("with")]),t._v(" tf"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Session"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{attrs:{class:"token keyword"}},[t._v("as")]),t._v(" sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{attrs:{class:"token comment"}},[t._v("# Load the weights and bias - No Error")]),t._v("\n    saver"),n("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("restore"),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sess"),n("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" save_file"),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{attrs:{class:"token keyword"}},[t._v("print")]),n("span",{attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{attrs:{class:"token string"}},[t._v("'Loaded Weights and Bias successfully.'")]),n("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),n("h3",{attrs:{id:"_4-正规化"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_4-正规化","aria-hidden":"true"}},[t._v("#")]),t._v(" 4. 正规化")]),n("p",[n("strong",[t._v("「紧身裤」"),n("strong",[t._v("问题：紧身裤非常合身，但是难以穿上，因此我们会穿稍大一点的裤子。如果数据非常符合模型，将难以优化，因此我们会选择一个更泛化的模型，来防止出现")]),t._v("过拟合")]),t._v("。")]),n("p",[t._v("防止过拟合的方法：")]),n("ul",[n("li",[t._v("过早终止")]),n("li",[t._v("正则化：对神经网络进行人为的约束，使得隐式的减少参数个数。\n"),n("ol",[n("li",[t._v("L2")]),n("li",[t._v("dropout")])])])]),n("h4",{attrs:{id:"_1-l2-正则化"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_1-l2-正则化","aria-hidden":"true"}},[t._v("#")]),t._v(" 1. L2 正则化")]),n("p",[t._v("L2 正则化非常简单，我们只需要在loss函数上加一部分，即所有向量的平方和/2，而不需要修改模型的结构，而且它的导数也非常的简单。")]),n("p",[n("img",{attrs:{src:a(177),alt:""}})]),n("p",[n("img",{attrs:{src:a(176),alt:""}})]),n("h4",{attrs:{id:"_2-dropout正则化"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_2-dropout正则化","aria-hidden":"true"}},[t._v("#")]),t._v(" 2. Dropout正则化")]),n("p",[t._v("将训练的样本随机取一半设置为0，使的网络不依赖任何给定的激活存在，因为任何激活都可能被摧毁")]),n("p",[n("img",{attrs:{src:a(175),alt:""}}),t._v("\n因此网络需要储存不同表示方法的冗余的数据\n"),n("img",{attrs:{src:a(174),alt:""}}),t._v("\n这种方法看上去有些啰嗦，实际上却可以增强网络的可靠性，并且能够防止"),n("strong",[t._v("过拟合")])]),n("p",[n("img",{attrs:{src:a(173),alt:""}})]),n("p",[t._v("The "),n("code",[t._v("tf.nn.dropout()")]),t._v(" function takes in two parameters:")]),n("ol",[n("li",[n("code",[t._v("hidden_layer")]),t._v(": the tensor to which you would like to apply dropout")]),n("li",[n("code",[t._v("keep_prob")]),t._v(": the probability of keeping (i.e. not dropping) any given unit")])]),n("p",[t._v("keep_prob allows you to adjust the number of units to drop. In order to compensate for dropped units, tf.nn.dropout() multiplies all units that are kept (i.e. not dropped) by 1/keep_prob.")]),n("p",[t._v("During training, a good starting value for keep_prob is 0.5.")]),n("p",[t._v("During testing, use a keep_prob value of 1.0 to keep all units and maximize the power of the model")]),n("div",{staticClass:"tip custom-block"},[n("p",{staticClass:"custom-block-title"},[t._v("扩展阅读")])])])}],o=a(0),e=Object(o.a)({},function(){this.$createElement;this._self._c;return this._m(0)},n,!1,null,null,null);s.default=e.exports}}]);